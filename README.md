# who_made_it
This tries to determine whether a given poem is real or was generated by AI or by an amateur

The included notebook uses fine tuning to construct an Open AI model that determines whether a given poem is real.

The idea is to start with 19 poems written in 2023.  Because they are written recently, they cannot be in Open AI's database.  Therefore, Open AI cannot somehow look them up in its memory and"know" that they are real poems.  We then take the first 80 characters of each poem, and ask Open AI to extend it to become a longer poem.

Suppose we start with the first 70 characters of a pre-2023 poem, for the sake of having a clear example:

Two roads diverged in a yellow wood,  
And sorry I could not travel both  

The correct extension of the poem is:

Two roads diverged in a yellow wood,  
And sorry I could not travel both  
And be one traveler, long I stood  
And looked down one as far as I could  
To where it bent in the undergrowth;  

GPT provides the following creative extension:

Two roads diverged in a yellow wood,  
And sorry I could not travel both  
Yet each beckoned with a promise to explore,  
One bathed in sunlight, the other veiled in shadow's oath.  
In contemplation, I stood at the crossroads' core.  

Now the two versions of the poem - Frost's and GPT's - would become training data for a fine tuning job.  The fine tuning would start from the model gpt-3.5-turbo and would modify it so as to be especially proficient at distinguishing the two verisons.  The new model, when asked which poem (Frost's or GPT's) is written by a real poet, would be likely to get the right answer.  By feeding many test cases into the fine tuning job, we make it more likely that the fine-tuned model can guess a wide variety of poems.

Although you can't see it in the notebook, I'll note that the "real" poems contain a small amount of profanity, as modern poems are wont to do.  It's important to be able to use datasets that have this kind of problem, because in real life Large Language Models are fed with all kinds of data.  In real use cases, we cannot simply choose to use "perfect" data sets.

In our case, the profanity does not affect the user-viewed output of the model, which simply judges 0 or 1, AI-generated or real poet.  So it is not a major issue.  Nevertheless, I wanted to practice removing the profanity from the poems, so I have done so in the notebok.  In real applications of LLMs, all kinds of data are possible, and one must be careful about what the user can and can't see.

I am only using 20 training cases and 18 validation cases, because the bank of 2023 poems I found is small.  If I were to do this more seriously, I would want at least 50 training cases, ideally more.

I have suggested to the model that the number of cliches in the poem is a good clue toward its authenticity.  I noticed that GPT-generated poems tend to be full of cliches.  When I left this instruction out, GPT was not able to reliably distinguish between authentic and inauthentic poems.  In real applications, it is important to use domain knowledge and not just rely on models to figure everything out "from scratch."

There is an open question as to how much of the work is being done by the fine tuning, and how much is being done by the specific instruction to search for cliches.  I will leave this question for future work.  I was interested here in practicing fine-tuning, regardless of whether its use is really justified in a practical sense.

In any case, the fine tuned model is extremely accurate; it gets only one wrong answer out of 18 validation cases.  I've also checked it with a few poems that I wrote (which it can often mark as AI/amateur) and a few real poems external to the data set (which it seems to mark as "real poet.")  With real poems written prior to 2022, there is a concern that it may be simply noting that this is a known poem written by e.g. Robert Frost.  That's why the 2023 dataset is so important.
